---
title: "MDP - Markov Decision Processes"
author: "Michael Hahsler"
format: 
  html: 
    theme: default
    toc: true
    number-sections: true
    code-line-numbers: true
    embed-resources: true
---

![CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/3.0/88x31.png) This code is provided under [Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License.](https://creativecommons.org/licenses/by-sa/4.0/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
```

# Introduction

AIMA chapter 17 is about _sequential decision problems_ where the agent's utility depends on a sequence of decisions. We will implement the 
key concepts using R for the AIMA 3x4 grid world example. 

More details can be found in 
[Reinforcement Learning: An Introduction (RL)](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto (2020)
in Chapter 4: Dynamic Programming.

**Note** that the code in this notebook defines explicit functions 
matching the textbook definitions and is for demonstration purposes only. Efficient implementations for larger problems use fast vector multiplications
instead. 

# Markov Decision Processes

MDPs are sequential decision problems with

-   a fully observable, stochastic environment,
-   a Markovian transition model, and
-   additive rewards.

MDPs are defined by:

-   A set of states $S$ with an initial state $s_0$.
-   A set of available $\mathrm{actions}(s)$ in each state.
-   A transition model $P(s'|s,a)$ to define how we move between states depending on actions.
-   A reward function $R(s, a, s')$ defined on state transitions and the actions taken.

A **policy** $\pi = \{\pi(s_0), \pi(s_1), \dots\}$ defines for each state which action to take. If we assume that under policy $\pi$, the agent
will go through the state sequence
$s_0, s_1, ..., s_\infty$, then the expected utility of being in state
$s_0$ can be calculated as a sum. To incorporate that earlier rewards
are more important, a discount factor $\gamma$ is used.

$U^\pi = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R(s_t, \pi(s_t), s_{t+1})\right]$

The goal of solving a MDP is to find an optimal policy that maximizes the expected future utility.

$\pi^*(s) = \mathrm{argmax}_\pi U^\pi(s)$ for all $s \in S$

{{< include _AIMA-4x3-gridworld.qmd >}}

# Solution Methods

It is convenient for solving MDPs using state-action values so we first 
define the Q-function.

## Q-Function

Let's define the expected utility of a state given that the agent always
chooses the optimal action (which it hopefully will if it is rational).

$U(s) = \max_{a \in A(s)}\sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma U(s')]$

This equation is called the _Bellman equation_ resulting in an equation system
with one equation per state $s$. This system of equations is hard to solve
for all $U(s)$ values because of the nonlinear $\max()$ operator.

Lets define a function for the expected utility of any possible
action $a$ (not just the optimal one) in a given state $s$. This is
called the (Q-function or state-action value function):

$Q(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma U(s')]$

This function is convenient for solving MDPs and can easily be implemented.

```{r}
Q_value <- function(s, a, U) {
  if(!(a %in% actions(s))) return(NA)
  
  sum(sapply(
    S,
    FUN = function(sp)
      P(sp, s, a) * (R(s, a, sp) + GAMMA * U[sp])
  ))
}
```

The issue is that we need to know $U$
representing the expected utility of a state given optimal decisions
is needed. Value iteration uses a simple iterative 
algorithm to solve this problem by successively updating `Q` and `U`.

Note that $U(s) = \max_a Q(s, a)$ holds and we get:

$Q(s,a) = \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma \max_{a'} Q(s', a')]$

## Value Iteration

The goal is to find the unique utility function $U$ (a vector of 
utilities, one for each state) for the MDP and
then derive the implied optimal policy $\pi^*$.

**Algorithm:** Start with a $U(s)$ vector of 0 for all states and then
update (Bellman update) the vector iteratively until it converges. This
procedure is guaranteed to converge to the unique optimal solution. The pseudocode
from AIMA Figure 17.6.:

![AIMA Figure 17.6: The value iteration algorithm for calculating utilities of states.](figures/AIMA_Figure_17_6.png)

**Stopping criterion:**
$||U^\pi - U||_\infty$ is called the _policy loss_ (i.e., the most the agent can loose by using policy $\pi$ instead of 
the optimal policy $\pi^*$ implied in $U$).
The max-norm $||x||_\infty$ is defined as the largest component of a 
vector $x$. 

It can be shown that if
$||U_{i+1} - U_i||_\infty < \epsilon(1-\gamma)/\gamma$ then
$||U_{i+1} - U||_\infty < \epsilon$. This can be used as a stopping 
criterion with guarantee of a policy loss of less than $\epsilon$.

```{r}
value_iteration <- function(eps, verbose = FALSE) {
  U_prime <- rep(0, times = length(S))
  i <- 1L
   
  while (TRUE) {
    if(verbose) cat("Iteration:", i)
    #cat("U:", U_prime, "\n")
    
    U <- U_prime
    delta <- 0
    
    for (s in S) {
      U_prime[s] <- max(sapply(
        actions(s),
        FUN = function(a)
          Q_value(s, a, U)
      ))
      delta <- max(delta, abs(U_prime[s] - U[s]))
    }
    
    if(verbose) cat(" -> delta:", delta, "\n")
    
    if (delta <= eps * (1 - GAMMA) / GAMMA)
      break
    
  i <- i + 1L  
  }
  
  cat("Iterations needed:", i, "\n")
  
  U
}
```

```{r}
U <- value_iteration(eps = 1e-6)
show_layout(U)
```

For the optimal policy, we choose in each state the action that
maximizes the expected utility. This is called the maximum expected utility (MEU) policy. The action that maximizes the utility can be found using the 
Q-function.

$\pi^*(s) = \mathrm{argmax}_a Q(s, a)$

For state 1, `'Up'` is the best move

```{r}
sapply(A, FUN = function(a) Q_value(s = 1, a, U = U))
```

Calculate the Q-function for all $S \times A$.

```{r}
Q_value_vec <- Vectorize(Q_value, vectorize.args = c("s", "a"))

QVs <- outer(S, A, FUN = function(s, a) Q_value_vec(s, a, U = U))
colnames(QVs) <- A
QVs
```

The optimal policy is the greedy policy that always picks the action with 
the largest Q-value.

```{r}
pi_star <- A[apply(QVs, MARGIN = 1, which.max)]
show_layout(pi_star)
```

Estimate the expected utility using simulation.

```{r}
utility_opt <- simulate_utilities(pi_star)

# expected utility
mean(utility_opt)
hist(utility_opt, xlim = c(-1, 1))
```

Compare three policies.

```{r}
c(
  random = mean(utility_random), 
  manual = mean(utility_manual), 
  opt = mean(utility_opt))
```

Since we know that utility_opt is very close to $U$, we can estimate the 
_policy loss_ (i.e., the most the agent can loose by using 
$\pi$ instead of $\pi*$) of the other policies given by:

$||U^\pi - U||_\infty$

Here is the policy loss for the manual policy. The maximum norm is the 
component with the largest difference. First, we calculate the absolute difference for each state.

```{r}
show_layout(abs(U_manual - U)) 
```

The maximum is:

```{r}
max(abs(U_manual - U))
which.max(abs(U_manual - U))
```

The policy loss is driven by the bad action taken in state 10 which is at coordinate (1, 4).

## Policy Iteration

Policy iteration tries to directly find the optimal policy. It alternates
between two steps:

1. **Policy evaluation:** given a current policy $\pi_i$, calculate $U^{\pi_i}$.
2. **Policy improvement:** calculate a new MEU policy $\pi_{i+1}$.

The pseudocode from AIMA Figure 17.9.:

![AIMA Figure 17.9: The policy iteration algorithm for calculating an optimal policy.](figures/AIMA_Figure_17_9.png)

For policy evaluation, we need to solve:

$U_i(s) = \sum_{s'} P(s'|s, \pi_i(s))[R(s, \pi_i(s), s') + \gamma U_i(s')]$

This is slightly simpler than the general Bellman equation, since 
the action in each state is fixed by the policy and there is no non-linear $\max()$ operator. For small state spaces this can be solved fast using a LP in $O(n^3)$.

For large state spaces, we can do approximate policy evaluation
by performing only a few iterations of a simplified Bellman update:

$U_{i+1}(s) \leftarrow \sum_{s'} P(s'|s, \pi_i(s))[R(s, \pi_i(s), s') + \gamma U_i(s')]$

We implement here approximate policy evaluation with `N` iterations.

```{r}
approx_policy_evaluation <- function(pi, U = NULL, N = 10) {
  # start with all 0s if no previous U is given
  if (is.null(U))
    U <- rep(0, times = length(S))
  
  for (i in seq_len(N)) {
    for (s in S) {
      U[s] = sum(sapply(
        S,
        FUN = function(s_prime) {
          P(s_prime, s, pi[s]) * (R(s, pi[s], s_prime) + GAMMA * U[s_prime])
        }
      ))
    }
  }
  U
}
```

```{r}
approx_policy_evaluation(pi_random)
approx_policy_evaluation(pi_manual)
```

We will implement modified policy iteration. Modified means that we use
the approximate policy evaluation.

```{r}
policy_iteration <- function(N = 10) {
  U <- rep(0, times = length(S))
  pi <- create_random_deterministic_policy()
  
  while (TRUE) {
    U <- approx_policy_evaluation(pi, U, N)
    unchanged <- TRUE
    for (s in S) {
      actns <- actions(s)
      a <- actns[which.max(sapply(actns, FUN = function(a) Q_value(s, a, U)))]
      if (Q_value(s, a, U) > Q_value(s, pi[s], U)) {
        pi[s] <- a
        unchanged <- FALSE
      }
    }
    
    if(unchanged) break
  }
  pi
}
```

```{r}
pi_opt_policy_it <- policy_iteration()
show_layout(pi_opt_policy_it)
```

## Linear Programming

Rewriting the Bellman equations as an LP formulation
requires replacing the non-linear $\max()$ operation using 
additional constraints. The LP can be solved in polynomial time. In practice
this is too slow for larger problems. The dynamic programming solution above 
is typically more efficient, but it is also restricted to small problems. 

## Approximate Offline Methods

Reinforcement learning is discussed in AIMA Chapter 22.
