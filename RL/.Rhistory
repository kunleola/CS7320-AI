cat(paste0(" -> " , round(Q[s_t, a_t], 3), " (G = ", round(G, 3),
")\n"))
a_star <- greedy_action(s_t, Q)
available_actions <- length(actions(s_t))
if (verbose) {
cat(paste0("  - pi for state ", s_t, ":\n"))
print(pi[s_t, available_actions])
}
pi[s_t, available_actions] <-
epsilon / length(available_actions)
pi[s_t, a_star] <- pi[s_t, a_star] + (1 - epsilon)
if (verbose) {
print(pi[s_t, available_actions])
cat("\n")
}
}
}
}
list(Q = Q,
pi = pi)
}
ret <- MC_on_policy(N = 1000, epsilon = 0.1, verbose = FALSE)
ret
show_layout(ret$pi)
ret <- MC_on_policy(N = 2, epsilon = 0.1, verbose = T)
MC_on_policy <- function(N = 100, epsilon = 0.1, verbose = FALSE) {
# Initialize
pi <- create_random_epsilon_soft_policy(epsilon)
Q <-
matrix(0,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
Q_N <-  matrix(0L,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
# Loop through N episodes
for (e in seq(N)) {
# always start from the start state defined by the problem
s_0 <- start
ep <- simulate_episode(pi, s_0)
if (verbose) {
cat(paste("*** Episode", e, "***\n"))
print(ep)
}
G <- 0
for (i in rev(seq(nrow(ep)))) {
r_t_plus_1 <- ep$r[i]
s_t <- ep$s[i]
a_t <- ep$a[i]
G <- gamma * G  + r_t_plus_1
# Only update for first visit of a s/a combination
if (i < 2L || !any(s_t == ep$s[1:(i - 1L)] &
a_t == ep$a[1:(i - 1L)])) {
if (verbose)
cat(paste0(
"Update at step ",
i,
":\n",
"  - Q(",
s_t,
", ",
a_t,
"): ",
round(Q[s_t, a_t], 3)
))
# running average instead of averaging Returns lists.
Q[s_t, a_t] <-
(Q[s_t, a_t] * Q_N[s_t, a_t] + G) / (Q_N[s_t, a_t] + 1)
Q_N[s_t, a_t] <- Q_N[s_t, a_t] + 1L
if (verbose)
cat(paste0(" -> " , round(Q[s_t, a_t], 3), " (G = ", round(G, 3),
")\n"))
a_star <- greedy_action(s_t, Q)
available_actions <- length(actions(s_t))
if (verbose) {
cat(paste0("  - pi for state ", s_t, " is updated:\n"))
print(pi[s_t, ])
}
pi[s_t, available_actions] <-
epsilon / length(available_actions)
pi[s_t, a_star] <- pi[s_t, a_star] + (1 - epsilon)
if (verbose) {
print(pi[s_t, ])
cat("\n")
}
}
}
}
list(Q = Q,
pi = pi)
}
ret <- MC_on_policy(N = 1000, epsilon = 0.1, verbose = FALSE)
ret <- MC_on_policy(N = 2, epsilon = 0.1, verbose = T)
pi
res
ret
ret <- MC_on_policy(N = 2, epsilon = 0.1, verbose = T)
ret <- MC_on_policy(N = 1, epsilon = 0.1, verbose = T)
MC_on_policy <- function(N = 100, epsilon = 0.1, verbose = FALSE) {
# Initialize
pi <- create_random_epsilon_soft_policy(epsilon)
Q <-
matrix(0,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
Q_N <-  matrix(0L,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
# Loop through N episodes
for (e in seq(N)) {
# always start from the start state defined by the problem
s_0 <- start
ep <- simulate_episode(pi, s_0)
if (verbose) {
cat(paste("*** Episode", e, "***\n"))
print(ep)
}
G <- 0
for (i in rev(seq(nrow(ep)))) {
r_t_plus_1 <- ep$r[i]
s_t <- ep$s[i]
a_t <- ep$a[i]
G <- gamma * G  + r_t_plus_1
# Only update for first visit of a s/a combination
if (i < 2L || !any(s_t == ep$s[1:(i - 1L)] &
a_t == ep$a[1:(i - 1L)])) {
if (verbose)
cat(paste0(
"Update at step ",
i,
":\n",
"  - Q(",
s_t,
", ",
a_t,
"): ",
round(Q[s_t, a_t], 3)
))
# running average instead of averaging Returns lists.
Q[s_t, a_t] <-
(Q[s_t, a_t] * Q_N[s_t, a_t] + G) / (Q_N[s_t, a_t] + 1)
Q_N[s_t, a_t] <- Q_N[s_t, a_t] + 1L
if (verbose)
cat(paste0(" -> " , round(Q[s_t, a_t], 3), " (G = ", round(G, 3),
")\n"))
a_star <- greedy_action(s_t, Q)
if (verbose) {
cat(paste0("  - pi for state ", s_t, " is updated:\n"))
print(pi[s_t, ])
}
pi[s_t, actions(s_t)] <-
epsilon / length(actions(s_t))
pi[s_t, a_star] <- pi[s_t, a_star] + (1 - epsilon)
if (verbose) {
print(pi[s_t, ])
cat("\n")
}
}
}
}
list(Q = Q,
pi = pi)
}
ret <- MC_on_policy(N = 1000, epsilon = 0.1, verbose = FALSE)
ret
show_layout(ret$pi)
apply(ret$pi, MARGIN = 1, which.max)
A[apply(ret$pi, MARGIN = 1, which.max)]
show_layout(A[apply(ret$pi, MARGIN = 1, which.max)])
show_layout(A[apply(ret$pi, MARGIN = 1, which.max)])
ret
simulate_episode <- function(pi, s_0, a_0 = NULL, max_length = 100) {
# rows are s_t, a_t, r_t+1; row 1 is t = 0
episode <- data.frame(s = rep(NA_character_, max_length),
a = rep(NA_character_, max_length),
r = rep(NA_real_, max_length))
if (is.null(a_0))
a_0 <- next_action(pi, s_0)
s <- s_0
a <- a_0
r <- NA
i <- 1L # i == 1 means t == 0!
while (TRUE) {
if (is_terminal(s))
break
s_prime <- sample_transition(s, a)
r <- R(s, a, s_prime)
episode[i, ] <- data.frame(s, a, r)
if (is_terminal(s_prime))
break
if (i >= max_length)
break
s <- s_prime
a <- next_action(pi, s)
i <- i + 1L
}
episode[1:i, ]
}
pi <- create_random_epsilon_soft_policy()
simulate_episode(pi, 1, 'Up')
simulate_episode <- function(pi, s_0, a_0 = NULL, max_length = 100) {
# rows are s_t, a_t, r_t+1; row 1 is t = 0
episode <- data.frame(t = 0:(max_length - 1L),
s = rep(NA_character_, max_length),
a = rep(NA_character_, max_length),
r = rep(NA_real_, max_length))
if (is.null(a_0))
a_0 <- next_action(pi, s_0)
s <- s_0
a <- a_0
r <- NA
i <- 1L # i == 1 means t == 0!
while (TRUE) {
if (is_terminal(s))
break
s_prime <- sample_transition(s, a)
r <- R(s, a, s_prime)
episode[i, ] <- data.frame(s, a, r)
if (is_terminal(s_prime))
break
if (i >= max_length)
break
s <- s_prime
a <- next_action(pi, s)
i <- i + 1L
}
episode[1:i, ]
}
pi <- create_random_epsilon_soft_policy()
simulate_episode(pi, 1, 'Up')
0:1
simulate_episode <- function(pi, s_0, a_0 = NULL, max_length = 100) {
# rows are s_t, a_t, r_t+1; row 1 is t = 0
episode <- data.frame(t = rep(NA_integer_, max_length),
s = rep(NA_character_, max_length),
a = rep(NA_character_, max_length),
r = rep(NA_real_, max_length))
if (is.null(a_0))
a_0 <- next_action(pi, s_0)
s <- s_0
a <- a_0
r <- NA
i <- 1L # i == 1 means t == 0!
while (TRUE) {
if (is_terminal(s))
break
s_prime <- sample_transition(s, a)
r <- R(t = i - 1L, s, a, s_prime)
episode[i, ] <- data.frame(s, a, r)
if (is_terminal(s_prime))
break
if (i >= max_length)
break
s <- s_prime
a <- next_action(pi, s)
i <- i + 1L
}
episode[1:i, ]
}
pi <- create_random_epsilon_soft_policy()
simulate_episode(pi, 1, 'Up')
simulate_episode <- function(pi, s_0, a_0 = NULL, max_length = 100) {
# rows are s_t, a_t, r_t+1; row 1 is t = 0
episode <- data.frame(t = rep(NA_integer_, max_length),
s = rep(NA_character_, max_length),
a = rep(NA_character_, max_length),
r = rep(NA_real_, max_length))
if (is.null(a_0))
a_0 <- next_action(pi, s_0)
s <- s_0
a <- a_0
r <- NA
i <- 1L # i == 1 means t == 0!
while (TRUE) {
if (is_terminal(s))
break
s_prime <- sample_transition(s, a)
r <- R(s, a, s_prime)
episode[i, ] <- data.frame(t = i - 1L, s, a, r)
if (is_terminal(s_prime))
break
if (i >= max_length)
break
s <- s_prime
a <- next_action(pi, s)
i <- i + 1L
}
episode[1:i, ]
}
pi <- create_random_epsilon_soft_policy()
simulate_episode(pi, 1, 'Up')
pi <- create_random_epsilon_soft_policy(epsilon = 0.1)
pi
simulate_episode(pi, 1, 'Up')
create_random_epsilon_soft_policy(epsilon = .2)
create_random_epsilon_soft_policy(epsilon = .5)
create_random_epsilon_soft_policy(epsilon = .9)
create_random_policy()
create_random_policy <-
function()
sapply(
S,
FUN = function(s)
sample(actions(s), 1L)
)
set.seed(1234)
pi_random <- create_random_policy()
random_pi
create_random_policy <-
function()
sapply(
S,
FUN = function(s)
sample(actions(s), 1L)
)
set.seed(1234)
pi_random <- create_random_policy()
pi_random
show_layout(pi_random)
create_random_policy <-
function()
structure(sapply(
S,
FUN = function(s)
sample(actions(s), 1L)
), names = S)
set.seed(1234)
pi_random <- create_random_policy()
pi_random
show_layout(pi_random)
ls()
greedy_policy <- function(Q) {
structure(apply(Q, MARGIN = 1, which.max), names = S)
}
Q
greedy_policy((Q)
)
greedy_policy(Q)
greedy_policy <- function(Q) {
structure(A[apply(Q, MARGIN = 1, which.max)], names = S)
}
greedy_policy(Q)
create_random_epsilon_soft_policy
make_policy_soft <-
function(pi, epsilon = 0.1) {
if(!is.vector(pi))
stop("pi is not a deterministic policy!")
p <-
matrix(0,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
for (s in S) {
p[s, actions(s)] <- epsilon / length(actions(s))
p[s, pi(s)] <- p[s, pi(s)] + (1 - epsilon)
}
p
}
pi
create_random_policy()
p <- create_random_policy()
make_policy_soft(p)
make_policy_soft <-
function(pi, epsilon = 0.1) {
if(!is.vector(pi))
stop("pi is not a deterministic policy!")
p <-
matrix(0,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
for (s in S) {
p[s, actions(s)] <- epsilon / length(actions(s))
p[s, pi[s]] <- p[s, pi[s]] + (1 - epsilon)
}
p
}
make_policy_soft(p)
make_policy_soft(pi_random)
Q[s_t, a_t] <-
Q[s_t, a_t] + (W / C[s_t, a_t]) * (G - Q[s_t, a_t])
MC_off_policy <-
function(N = 100,
epsilon = 0.1,
gamma = 1,
verbose = FALSE) {
# Initialize
Q <-
matrix(0,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
C <-  matrix(0L,
nrow = length(S),
ncol = length(A),
dimnames = list(S, A))
pi <- greedy_policy(Q)
# Loop through N episodes
for (e in seq(N)) {
b <- create_random_epsilon_soft_policy(epsilon)
s_0 <- start
ep <- simulate_episode(b, s_0)
if (verbose) {
cat(paste("*** Episode", e, "***\n"))
print(ep)
}
G <- 0
W <- 1
for (i in rev(seq(nrow(ep)))) {
r_t_plus_1 <- ep$r[i]
s_t <- ep$s[i]
a_t <- ep$a[i]
G <- gamma * G  + r_t_plus_1
C[s_t, a_t] <- C[s_t, a_t] + W
Q[s_t, a_t] <-
Q[s_t, a_t] + (W / C[s_t, a_t]) * (G - Q[s_t, a_t])
pi[s_t] <- greedy_action(s_t, Q)
if (a_t != pi[s_t])
break
W <- W * 1 / b[s_t, a_t]
}
}
list(Q = Q,
pi = pi)
}
ret <- MC_off_policy(N = 1000, epsilon = 0.1, verbose = FALSE)
ret
show_layout(ret$pi)
actions(1)
actions(1:2)
sample_MDP(maze, n = 1, horizon = 50)
? ? solve_MDP
if (!"markovDP" %in% rownames(installed.packages()))
install.packages('markovDP', repos = c(
'https://mhahsler.r-universe.dev',
'https://cloud.r-project.org'))
library(markovDP)
maze_dir <- system.file("mazes", package = "markovDP")
dir(maze_dir)
maze <- gw_read_maze(file.path(maze_dir, "L_maze.txt"))
maze
#| fig-width: 8
#| fig-height: 8
gw_plot(maze)
#| fig-width: 8
#| fig-height: 8
maze$transition_prob
gw_plot_transition_graph(maze)
maze$reward
#| fig-width: 8
#| fig-height: 8
system.time(sol <- solve_MDP(maze, method = "value_iteration"))
sol
gw_plot(sol)
gw_path(sol)
policy(sol)
#| fig-width: 8
#| fig-height: 8
gw_animate(maze, "value_iteration", n = 25, zlim = c(-1,100))
system.time(sol <- solve_MDP(maze, method ="q_learning",
horizon = 1000, n = 1000, alpha = 0.1))
sol
#| fig-width: 8
#| fig-height: 8
gw_plot(sol)
sample_MDP(maze, n = 1, horizon = 50)
sample_MDP(maze, n = 1, horizon = 50, trajectories = TRUE)
sample_MDP(maze, n = 1, horizon = 50, trajectories = TRUE)$trajectory
sample_MDP(maze, n = 1, horizon = 50, trajectories = TRUE)$trajectories
set.seed(1111)
sample_MDP(maze, n = 1, horizon = 50, trajectories = TRUE)$trajectories
set.seed(1111)
sample_MDP(maze, n = 1, horizon = 50, epsilon = 0.2, trajectories = TRUE)$trajectories
set.seed(1111)
sample_MDP(maze, n = 1, horizon = 50, trajectories = TRUE)$trajectories
#| fig-width: 8
#| fig-height: 8
sol <- gw_animate(maze, "q_learning", n = 25, zlim = c(-1,100),  horizon = 1000)
#| fig-width: 8
#| fig-height: 8
sol <- gw_animate(maze, "q_learning", n = 25, zlim = c(-1,100),  horizon = 1000, alpha = 0.2)
system.time(sol <- solve_MDP(maze, method ="q_learning",
horizon = 1000, n = 500, alpha = 0.2))
sol
#| fig-width: 8
#| fig-height: 8
gw_plot(sol)
system.time(sol <- solve_MDP(maze, method ="q_learning",
horizon = 1000, n = 500, alpha = 0.3))
sol
#| fig-width: 8
#| fig-height: 8
gw_plot(sol)
system.time(sol <- solve_MDP(maze, method ="q_learning",
horizon = 1000, n = 100, alpha = 0.3))
sol
#| fig-width: 8
#| fig-height: 8
gw_plot(sol)
system.time(sol <- solve_MDP(maze, method ="q_learning",
horizon = 1000, n = 100, alpha = 0.3))
sol
#| fig-width: 8
#| fig-height: 8
gw_plot(sol)
#| fig-width: 8
#| fig-height: 8
sol <- gw_animate(maze, "q_learning", n = 25, zlim = c(-1,100),  horizon = 1000, alpha = 0.3)
